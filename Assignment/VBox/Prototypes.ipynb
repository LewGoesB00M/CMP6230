{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine \n",
    "import sklearn as skl\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import redis\n",
    "from direct_redis import DirectRedis\n",
    "\n",
    "import great_expectations as gx\n",
    "\n",
    "# MLFlow import\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "# Set up logging of important data\n",
    "import logging\n",
    "logging.basicConfig(level=logging.WARN)\n",
    "logger = logging.getLogger(__name__)\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42\n",
    "DEFAULT_CONN_STRING = \"mysql+pymysql://lewis:MLOps6230@172.17.0.2:3306/LoanApproval\"\n",
    "REDIS_CONN_INFO = {\n",
    "    \"host\": \"localhost\", \n",
    "    \"port\": 6379, \n",
    "    \"db\": 0, \n",
    "#     \"trainX\": \"trainX\",\n",
    "#     \"trainY\": \"trainY\", \n",
    "#     \"testX\": \"testX\",\n",
    "#     \"testY\": \"testY\", # Train and test are eventually replaced with the train and test sets.\n",
    "    \"table\": \"LoanApproval\"}\n",
    "\n",
    "DEFAULT_PATH = \"~/CMP6230/loan_data.csv\"\n",
    "# DEFAULT_COLUMNS = [\"person_age\",\"person_gender\",\"person_education\",\"person_income\",\"person_emp_exp\",\"person_home_ownership\",\"loan_amnt\",\"loan_intent\",\"loan_int_rate\",\"loan_percent_income\",\"cb_person_cred_hist_length\",\"credit_score\",\"previous_loan_defaults_on_file\",\"loan_status\"]\n",
    "DEFAULT_TABLE_NAME = \"LoanApproval\"\n",
    "GX_CONTEXT = gx.get_context()\n",
    "\n",
    "redis_conn = DirectRedis(host = REDIS_CONN_INFO[\"host\"], port = REDIS_CONN_INFO[\"port\"], db = REDIS_CONN_INFO[\"db\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_db_context(conn_string = DEFAULT_CONN_STRING):\n",
    "    return create_engine(conn_string)\n",
    "\n",
    "def extract_csv(path = DEFAULT_PATH): # column_names = DEFAULT_COLUMNS):\n",
    "    return pd.read_csv(path) # , names = column_names)\n",
    "\n",
    "def write_df_to_db(df, table_name = DEFAULT_TABLE_NAME, if_exists = \"replace\", conn_string = DEFAULT_CONN_STRING):\n",
    "    eng_conn = create_db_context(conn_string)\n",
    "    df.to_sql(table_name, con = eng_conn, if_exists = if_exists, index = False)\n",
    "    eng_conn.dispose()\n",
    "    \n",
    "def read_df_from_db(conn_string, table_name):\n",
    "    eng_conn = create_db_context(conn_string)\n",
    "    return pd.read_sql(table_name, conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transform_load(from_path = DEFAULT_PATH, to_conn = DEFAULT_CONN_STRING):\n",
    "    df_extracted = extract_csv(from_path)\n",
    "    # Transformation\n",
    "    df_extracted[\"person_income\"] = df_extracted[\"person_income\"].astype(int)\n",
    "    df_extracted[\"person_age\"] = df_extracted[\"person_age\"].astype(int)\n",
    "    df_extracted[\"loan_amnt\"] = df_extracted[\"loan_amnt\"].astype(int)\n",
    "    df_extracted[\"cb_person_cred_hist_length\"] = df_extracted[\"cb_person_cred_hist_length\"].astype(int)\n",
    "    \n",
    "    \n",
    "    write_df_to_db(df_extracted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(df):\n",
    "    # Encode the data using Label and One-Hot encoders.\n",
    "    le = LabelEncoder() # Label encoding converts strings to numbers, and is best suited to ordinal data.\n",
    "    # In this dataset, person_education is ordinal as a person's level of education goes in order of Bachelors, Masters, etc.\n",
    "    df[\"person_education\"] = le.fit_transform(df[\"person_education\"])\n",
    "    \n",
    "    # One-hot encoding also converts strings to numbers, but to do so it creates new Boolean columns for each\n",
    "    # of the original values in the column, removing the original column in the process.\n",
    "    encodedDf = pd.get_dummies(df, columns=df.select_dtypes(include=['object']).columns)\n",
    "#     print(encodedDf.columns)\n",
    "#     print(encodedDf.shape)\n",
    "    return encodedDf\n",
    "    \n",
    "def x_y_split(df, target):\n",
    "    # Split the dataset into X and Y sets, where the main data is X and the target column is Y.\n",
    "    x = df.drop(target, axis = 1)\n",
    "    y = df[target]\n",
    "    return x, y\n",
    "\n",
    "def oversample(x, y):\n",
    "    smote = SMOTE()\n",
    "    x = x.rename(str, axis=\"columns\")  # SMOTE doesn't work unless this line is added.\n",
    "    return smote.fit_resample(x, y)\n",
    "\n",
    "def train_test_splitter(x, y):\n",
    "    train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.2, random_state = RANDOM_STATE)\n",
    "    return train_x, test_x, train_y, test_y\n",
    "\n",
    "def preprocessing(from_conn, to_redis = REDIS_CONN_INFO):\n",
    "    eng_conn = create_db_context(from_conn)\n",
    "    df = read_df_from_db(from_conn, REDIS_CONN_INFO[\"table\"])\n",
    "    \n",
    "    # Preprocessing\n",
    "    # person_income varies wildly, so let's drop some of them.\n",
    "    outlierHighIncome  = df[\"person_income\"].quantile(0.98)\n",
    "    df = df[(df[\"person_income\"] < outlierHighIncome)]\n",
    "    \n",
    "    # Also remove rows with unlikely ages.\n",
    "    df = df[(df[\"person_age\"] < 85)]\n",
    "    \n",
    "    # Encode categorical rows.\n",
    "    df = encode(df)\n",
    "    \n",
    "    # Split into X and Y sets.\n",
    "    x, y = x_y_split(df, \"loan_status\")\n",
    "    \n",
    "    # Apply SMOTE to balance the dataset.\n",
    "    x, y = oversample(x, y)\n",
    "    \n",
    "    # Split the data into training and testing sets\n",
    "    train_x, test_x, train_y, test_y = train_test_splitter(x, y)\n",
    "\n",
    "    # Serialize the dataframes and send them to Redis to be reimported in model development\n",
    "    r = DirectRedis(host = REDIS_CONN_INFO[\"host\"], port = REDIS_CONN_INFO[\"port\"], db = REDIS_CONN_INFO[\"db\"])\n",
    "    \n",
    "#     train_buffer = pa.serialize_pandas(train_x)\n",
    "    r.set(\"trainX\", train_x)\n",
    "    \n",
    "#     test_buffer = pa.serialize_pandas(test_x)\n",
    "    r.set(\"testX\", test_x)\n",
    "\n",
    "#     y_table = pa.Table.from_pandas(train_y)\n",
    "#     output_stream = pa.BufferOutputStream()\n",
    "#     pq.write_table(y_table, output_stream)\n",
    "#     serialized_y = output_stream.getvalue().to_pybytes()\n",
    "#     serialized_y = train_y.to_pickle()\n",
    "    r.set(\"trainY\", train_y)\n",
    "    \n",
    "#     y_table = pa.Table.from_pandas(test_y)\n",
    "#     output_stream = pa.BufferOutputStream()\n",
    "#     pq.write_table(y_table, output_stream)\n",
    "#     serialized_y = output_stream.getvalue().to_pybytes()\n",
    "    r.set(\"testY\", test_y)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(params, from_redis = REDIS_CONN_INFO):\n",
    "    r = DirectRedis(host = REDIS_CONN_INFO[\"host\"], port = REDIS_CONN_INFO[\"port\"], db = REDIS_CONN_INFO[\"db\"])\n",
    "    \n",
    "    # Deserialize the dataframes from Redis.\n",
    "    train_x = r.get(\"trainX\")\n",
    "#     train_x = pa.deserialize_pandas(train_x_buffer)\n",
    "    \n",
    "    test_x = r.get(\"testX\")\n",
    "#     test_x = pa.deserialize_pandas(test_x_buffer)\n",
    "    \n",
    "    train_y = r.get(\"trainY\")\n",
    "#     train_y = pa.deserialize_pandas(train_y_buffer)\n",
    "    \n",
    "    test_y = r.get(\"testY\")\n",
    "#     test_y = pa.deserialize_pandas(test_y_buffer)\n",
    "    \n",
    "    # Scale the data.\n",
    "    sc = StandardScaler()\n",
    "    train_x = sc.fit_transform(train_x)\n",
    "    test_x = sc.transform(test_x)\n",
    "    \n",
    "    # Start MLFlow.\n",
    "    run = mlflow.start_run()\n",
    "    \n",
    "    mdl = RandomForestClassifier(n_estimators = params[\"n_estimators\"])\n",
    "    mdl.fit(train_x, train_y)\n",
    "\n",
    "    # Pickle is used to seralize the model.\n",
    "    serialized_mdl = pkl.dumps(mdl)\n",
    "    r.set(\"LoanApproval_trained_mdl\", serialized_mdl)\n",
    "\n",
    "    r.set(\"LoanApproval_trained_params\", params)\n",
    "    \n",
    "    r.set(\"LoanApproval_trained_run_id\", run.info.run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metrics(actual, pred):\n",
    "    accuracy = accuracy_score(actual, pred)\n",
    "    precision = precision_score(actual, pred)\n",
    "    f1 = f1_score(actual, pred)\n",
    "    \n",
    "    return accuracy, precision, f1\n",
    "    \n",
    "def evaluate(from_redis = REDIS_CONN_INFO):\n",
    "    r = DirectRedis(host = REDIS_CONN_INFO[\"host\"], port = REDIS_CONN_INFO[\"port\"], db = REDIS_CONN_INFO[\"db\"])\n",
    "    \n",
    "    tmp = r.get(\"LoanApprovalModel\")\n",
    "    print(tmp)\n",
    "    \n",
    "    mdl = pkl.loads(r.get(\"LoanApproval_trained_mdl\"))\n",
    "        \n",
    "    # Getting the parameters\n",
    "    params = r.get(\"LoanApproval_trained_params\")\n",
    "    n_estimators = params[\"n_estimators\"]\n",
    "    # Another parameter might be good.\n",
    "\n",
    "    # Getting the run id for MLFlow to continue the experiment\n",
    "    run_id = r.get(\"LoanApproval_trained_run_id\")\n",
    "\n",
    "    # Resume the previously started experiment run\n",
    "    # run = mlflow.start_run(run_id=run_id)\n",
    "\n",
    "    # Perform the prediction\n",
    "    test_x = r.get(\"testX\")\n",
    "    test_y = r.get(\"testY\")\n",
    "    predicted_qualities = mdl.predict(test_x)\n",
    "\n",
    "    # Evaluate the results     \n",
    "    accuracy, precision, f1 = eval_metrics(test_y, predicted_qualities)\n",
    "\n",
    "    # Output the metrics to the stdout   \n",
    "    # (this is somewhat equivalent to print)\n",
    "    sys.stdout.write(\"Random Forest model (n_estimators: %f):\\n\" % (n_estimators))\n",
    "    sys.stdout.write(\"  Accuracy: %s\\n\" % accuracy)\n",
    "    sys.stdout.write(\"  Precision: %s\\n\" % precision)\n",
    "    sys.stdout.write(\"  F1: %s\\n\" % f1)\n",
    "\n",
    "    # Log the parameters to mlflow\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "\n",
    "    # Log the metrics to mlflow\n",
    "    mlflow.log_metric(\"accuracy\", accuracy) \n",
    "    mlflow.log_metric(\"precision\", precision) \n",
    "    mlflow.log_metric(\"f1\", f1)     \n",
    "    tracking_url_type_filestore = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "    # model logging\n",
    "    # Model registry does not work with file store\n",
    "    if tracking_url_type_filestore != \"file\":            \n",
    "        # Registering the model\n",
    "        # Please refer to the documentation for further information:\n",
    "        # https://mlflow.org/docs/la/model-registry.html#api-workflow\n",
    "        mlflow.sklearn.log_model(mdl, \"model\", registered_model_name=\"LoanApproval_RandomForestModel\")\n",
    "    else:\n",
    "        mlflow.sklearn.log_model(mdl, \"model\")\n",
    "\n",
    "     # End the current experiment run\n",
    "    mlflow.end_run()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lewis/miniconda3/envs/Pipeline/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/home/lewis/miniconda3/envs/Pipeline/lib/python3.9/site-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n",
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "/home/lewis/miniconda3/envs/Pipeline/lib/python3.9/site-packages/sklearn/utils/validation.py:2732: UserWarning: X has feature names, but RandomForestClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest model (n_estimators: 200.000000):\n",
      "  Accuracy: 0.8159818686942536\n",
      "  Precision: 0.7331707834199088\n",
      "  F1: 0.8460456297021225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/12/11 18:46:33 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n",
      "Registered model 'LoanApproval_RandomForestModel' already exists. Creating a new version of this model...\n",
      "2024/12/11 18:46:35 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LoanApproval_RandomForestModel, version 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸƒ View run melodic-fox-654 at: http://localhost:5000/#/experiments/0/runs/76b714a5f30440dc981eba281e408bb6\n",
      "ðŸ§ª View experiment at: http://localhost:5000/#/experiments/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '2' of model 'LoanApproval_RandomForestModel'.\n"
     ]
    }
   ],
   "source": [
    "extract_transform_load()\n",
    "preprocessing(DEFAULT_CONN_STRING)\n",
    "training(params = {\"n_estimators\": 200})\n",
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = read_df_from_db(DEFAULT_CONN_STRING, \"LoanApproval\")\n",
    "df2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
