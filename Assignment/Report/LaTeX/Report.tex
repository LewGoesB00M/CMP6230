% -------------------------------------------------------------------------------
% Establish page structure & font.
\documentclass[12pt]{report}

\usepackage[total={6.5in, 9in},
	left=1in,
	right=1in,
	top=1in,
	bottom=1in,]{geometry} % Page structure

\usepackage{graphicx} % Required for inserting images
\graphicspath{{../../.images/}} % Any additional images I use (BCU logo, etc) are from here.

\usepackage[utf8]{inputenc} % UTF-8 encoding
\usepackage[T1]{fontenc} % T1 font
\usepackage{float}  % Allows for floats to be positioned using [H], which correctly
                    % positions them relative to their location within my LaTeX code.
\usepackage{subcaption}

% -------------------------------------------------------------------------------
% Declare biblatex with custom Harvard BCU styling for referencing.
\usepackage[
    useprefix=true,
    maxcitenames=3,
    maxbibnames=99,
    style=authoryear,
    dashed=false, 
    natbib=true,
    url=false,
    backend=biber
]{biblatex}

% Additional styling options to ensure Harvard referencing format.
\renewbibmacro*{volume+number+eid}{
    \printfield{volume}
    \setunit*{\addnbspace}
    \printfield{number}
    \setunit{\addcomma\space}
    \printfield{eid}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}

% Declaring both bib sources.
% Pipeline is the original one copied from draft pipeline, Report is new for this.
% This is because pipeline.bib is over a thousand lines long.
\addbibresource{Pipeline.bib}
\addbibresource{Report.bib}

% -------------------------------------------------------------------------------
% To prevent "Chapter N" display for each chapter
\usepackage[compact]{titlesec}
\usepackage{wasysym}
\usepackage{import}

\titlespacing*{\chapter}{0pt}{-2cm}{0.5cm}
\titleformat{\chapter}[display]
{\normalfont\bfseries}{}{0pt}{\Huge}

% -------------------------------------------------------------------------------
% Custom macro to make an un-numbered footnote.

\newcommand\blfootnote[1]{
    \begingroup
    \renewcommand\thefootnote{}\footnote{#1}
    \addtocounter{footnote}{-1}
    \endgroup
}

% -------------------------------------------------------------------------------
% Fancy headers; used to show my name, BCU logo and current chapter for the page.
\usepackage{fancyhdr}
\usepackage{calc}
\pagestyle{fancy}

\setlength\headheight{37pt} % Set custom header height to fit the image.

\renewcommand{\chaptermark}[1]{%
    \markboth{#1}{}} % Include chapter name.


% Lewis Higgins - ID 22133848           [BCU LOGO]                [CHAPTER NAME]
\lhead{Lewis Higgins - ID 22133848~~~~~~~~~~~~~~~\includegraphics[width=1.75cm]{BCU}}
\fancyhead[R]{\leftmark}

% ------------------------------------------------------------------------------
% Used to add PDF hyperlinks for figures and the contents page.

\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=black,
}

% ------------------------------------------------------------------------------
\usepackage{xcolor} 
\usepackage{colortbl}
\usepackage{longtable}
\usepackage{amssymb}
% ------------------------------------------------------------------------------
\usepackage{tcolorbox}
\newcommand{\para}{\vspace{7pt}\noindent}
% -------------------------------------------------------------------------------

\title{Data Management and MLOps Activities Log Report}
\author{Lewis Higgins - Student ID 22133848}
\date{December 2024}

% -------------------------------------------------------------------------------

\begin{document}


\makeatletter
\begin{titlepage}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{BCU}\\[4ex]
        {\huge \bfseries CMP6230 - Assignment 2}\\[2ex]
        {\large \bfseries  \@title}\\[50ex]
        {\@author}\\[2ex]
        {CMP6230 - Data Management and Machine Learning Operations}\\[2ex]
        {Module Coordinator: Sima Iranmanesh}\\[10ex]
    \end{center}
\end{titlepage}
\makeatother
\thispagestyle{empty}
\newpage


% Page counter trick so that the contents page doesn't increment it.
\setcounter{page}{0}

\tableofcontents
\thispagestyle{empty}

\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
% UPDATE INTRO BECAUSE THE REPORT ISN'T JUST A PLAN ANYMORE.
This report aims to design and document a five-stage Machine Learning Operations pipeline, from the initial 
data ingestion to the monitoring of the produced model, beginning 
with the identification of three candidate datasets for the pipeline to be conducted with, and continuing 
into descriptions of what each stage of the pipeline consists of, and how it will be implemented with the
one chosen dataset using a wide variety of software that is also used within industry. When this plan 
is implemented at a later stage, it will provide a clear framework to create an automated MLOps pipeline
with continuous integration and deployment.

% Importing the sections from the original report here.
\include{Datasets.tex}
\include{PipelineStages.tex}
% Side-note: I think this actually builds immensely faster because it's not recompiling those two every time,
% it just stores the processed version in the .aux files to be instantly reused.


% Apparently this can only go 500 words over the word count. You are already past that with the inclusion 
% of those two TeX files. She doesn't seem massively strict over it though, stating "it's just a module".
% 2012 + 3597 + This word count, unless you change either Datasets.tex or PipelineStages.tex.
% 5609 + This

\section{Data terminology}\label{sec:dataTerms} % Poor section title, consider something else.
This project must take into account four key elements of data handling these being data privacy, security, 
ethics and data protection laws. 

\begin{longtable}{ | p{0.2\textwidth} | p{0.7\textwidth} |}
    \hline
    \cellcolor{blue!25}Term & \cellcolor{blue!25}Description\\
    \hline
    Data privacy & An individual's ability to govern what happens to their personal data \autocite{cloudflare_what_nodate}. Personal data 
    refers to data that could be used to singularly identify someone \autocite{yang_big_2021}.\\
    \hline
    Data security & The protection of personal/sensitive information from unauthorized access, use or manipulation \autocite{ibm_security_2021}.
    This can be the physical protection of material such as hard drives, and software protections like encryption. \\
    \hline
    Data ethics & A blanket term encompassing the moral obligations of data collection, use and protection \autocite{harvard_business_school_5_2021}.
    Ethical data storage would mean that the data subjects willingly gave their data and can ask for its deletion at any point. 
    Mandated by data protection legislation. \\
    \hline 
    Big data ethics & The application of data ethics to massive datasets, concerning the implications of their collection and use on society as a whole
    \autocite{richards_big_2014}. \\ % Poor definition I think, maybe rewrite this one.
    \hline 
    General Data \newline Protection \newline Regulations (GDPR) & Legislation introduced in the EU in 2016, giving individuals significantly more 
    rights over their personal data. Data subjects most notably may request a copy of their data, the removal of their data, and to object to their data's 
    collection \autocite{ico_guide_2024}. Additionally, data must be kept secure using all available means. Companies who fail to adhere to these policies
    face significant sanctions of \euro10,000,000 or 2\% of the company's annual turnover for minor violations, or double this (\euro20,000,000, 4\% turnover)
    for major violations, whichever figure is higher. \\
    \hline
    Data Protection Act 2018 & When the UK left the EU, the GDPR no longer applied. Therefore, the UK adopted the GDPR as an amendment to its own
    Data Protection Act. There are minimal differences between the two, only that the UK's fines are in GBP rather than Euros, meaning fines go up 
    to \pounds17,500,000 or 4\% turnover \autocite{ico_enforcement_2024}. \\
    \hline
\caption{A brief overview of relevant data terminology.}\label{tab:dataTerms}
% Maybe it shouldn't be "brief". I think GDPR especially needs its own paragraph moreso than a table column.
\end{longtable}


\chapter{Implementing the MLOps Pipeline}
\section{Ethical and Legal Considerations}
The dataset was shared under an Apache 2.0 open-source licence, permitting access and modification with no restrictions
except crediting the original author \autocite{apache_apache_nodate}, meaning that there are not any ethical or legal 
considerations of note with this dataset's usage. Given that the chosen dataset also consists of synthetic data,
there are fewer necessary considerations for its use.

\subsection{Synthetic data}
% What is Synthetic Data and what are the common methods that are currently leveraged to attempt
% to perform Synthetic Data Generation?
This term has been frequently used throughout the report to describe the Loan Approval Classification Dataset, and refers to 
data that has been generated algorithmically rather than collected from a real source. However, the generation of the 
synthetic data does often depend on real data to identify patterns and trends, meaning that legislation such as GDPR is still relevant
in those scenarios because the generated data must be impossible to link back to the original subject \autocite{Lopez2022OnTL}.
An example of synthetic data generation is SMOTE, which was used in the generation of the loan dataset \autocite{zoppelleto_financial_nodate}, 
which creates synthetic data in a dataset based on the other rows as previously mentioned in Section \ref{sec:Dataset}. SMOTE will also be used in Section 
\ref{sec:ImpPreprocessing} to balance the dataset.

\subsection{Model drift}
Model drift is a phenomenon that affects deployed production machine learning models where changes in 
their environment over time affects their performance \autocite{nigenda_amazon_2022}. Model drift is not an 
immediate occurrence, and can take a long time to occur, emphasising the importance of continuous integration and deployment. 
Table \ref{tab:Drift} details the different kinds of model drift.

\begin{longtable}{ | p{0.2\textwidth} | p{0.4\textwidth} | p{0.3\textwidth} |}
    \hline
    \cellcolor{blue!25}Drift type & \cellcolor{blue!25}Description & \cellcolor{blue!25}Dataset example\\
    \hline
    Data drift & Changes in the distributions of the ingested data over time that eventually significantly differ from the model's training data \autocite{datacamp_understanding_nodate}.
    & In this dataset, interest rates and people's yearly income may change as time passes or economic inflation occurs.\\
    \hline
    Concept drift & Changes in the relationship between the input and target variables \autocite{nigenda_amazon_2022}. 
    & In this dataset, this is somewhat unlikely. However, it could occur if there is a significant housing or job market crash 
    that would cause more people to need loans, which would change the original relationships.\\
    \hline
\caption{The common types of model drift.}\label{tab:Drift}
\end{longtable}

\pagebreak % It's just barely on the same page as the table but I don't want it to be.
\para Model drift can be a significant issue that greatly impacts the performance of ML models if it is not quickly noticed, and can 
also be an ethical concern if the model is being used to influence decisions such as granting loans in this project. 
Therefore, combatting model drift through methods such as continuous monitoring is necessary to ensure that the model remains accurate 
and usable. This can be accomplished in this project through MLFlow's monitoring dashboard.

% REVIEW TASK SHEETS 2, 3 AND 4.
% NOT SURE OF THE SEQUENCE OF 3 AND 4, CHECK EXAMPLE WORK.
% Example 2 (SimonRoadknight) is 21,334 words, or 18,809 without its appendices and references.
% Example 3 (MihaiValentin) is 12,802 words.



\include{SoftwareInit}
% Elements of SoftwareInit could be placed in the appropriate pipeline section.

\section{Package imports}
Many packages need to be imported for the Airflow DAG of the pipeline to succeed. These can be found 
below, alongside descriptions in the form of Python comments, in Figures \ref{fig:PipelineDAGImports} and 
\ref{fig:PipelineFunctionsImports}. The use of these packages within pipeline functions is detailed 
in this report within the appropriate pipeline stage.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/.Code/PipelineDAGImports.png}
    \caption{The packages necessary to run the DAG itself.}
    \label{fig:PipelineDAGImports}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/.Code/PipelineDAGImports.png}
    \caption{The packages necessary to run the functions within each DAG.}
    \label{fig:PipelineFunctionsImports}
\end{figure}

\section{Data Ingestion}\label{sec:ImpIngestion}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/.Code/PipelineFunctions/Ingestion.png}
    \caption{The code for the entire Ingestion stage.}
    \label{fig:IngestionCode}
\end{figure}

\para This code is split across four functions that are called upon within the main 
ETL function. The ETL function will call on extract\_csv to read the dataset CSV from 
the path set as DEFAULT\_PATH. After this, some data type transformations are performed 
to ensure the optimal data types are used for each column. Finally, the transformed 
Pandas DataFrame is written to the MariaDB Columnstore instance hosted within the Docker 
container at port 3306.

\subsection{Validation with Great Expectations}
% !!! You can't do this section until you have a screenshot of the Great Expectations Operator installation.
\para Conda was used to install a helper package for Airflow that adds the Great Expectations Operator.
This operator connects to the checkpoint produced in \ref{fig:GXCheckpoint3} to run the saved validation 
methods on the ingested dataset without the need to declare it as a specialised Python function within 
the code. 

\section{Data Preprocessing}\label{sec:ImpPreprocessing}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/.Code/PipelineFunctions/Preprocessing1.png}
    \caption{The secondary functions used in the main Preprocessing function.}
    \label{fig:PreprocessingCode1}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/.Code/PipelineFunctions/Preprocessing2.png}
    \caption{The code for the main Preprocessing function.}
    \label{fig:PreprocessingCode2}
\end{figure}

\para This code forms the Data Preprocessing stage of the pipeline. Data is retrieved from the 
Columnstore and considerable outliers are removed, primarily from the Age and Income columns, 
where data can be outright false or differs enough to heavily skew the dataset and affect scaling.
After this, categorical data is encoded using label encoding and one-hot encoding where necessary.
The data is then balanced using SMOTE to resolve the significant imbalance between granted and denied loans 
before being split into training and testing sets, which are serialized and saved into Redis. The use of 
a DirectRedis connection rather than a standard Redis connection removes the need for explicit serialization, 
as it is automatically performed when "r.set()" is called. 

\section{Model Development}\label{sec:ImpDevelopment}
Internally, this section was referred to as "Training", seen in the function name and associated 
DAG task.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/.Code/PipelineFunctions/Training.png}
    \caption{The code for the Training function.}
    \label{fig:TrainingCode}
\end{figure}

The Training function deserializes and reads the preprocessed DataFrame from Redis via a Direct-Redis 
connection, and then scales the data using a StandardScaler from Scikit-Learn. MLFlow is started 
to begin its tracking of the model, and then the model is instantiated and trained on the preprocessed data 
using the parameters provided to the Training function. These parameters would be provided within the DAG code.
The model is then serialized using Pickle and stored into Redis, alongside the parameters for logging purposes 
and the MLFlow run ID so that the run can be resumed in the Evaluation phase.


\section{Model Evaluation}\label{sec:ImpEvaluation}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/.Code/PipelineFunctions/Evaluation1.png}
    \caption{eval\_metrics, which returns the evaluation metrics of a model.}
    \label{fig:Evaluation1}
\end{figure}

\para The eval\_metrics function calculates the accuracy, precision and F1 scores of the trained model 
using predefined functions that are part of Scikit-Learn. This is later used in the main Evaluation function 
to log these scores in MLFlow.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/.Code/PipelineFunctions/Evaluation2.png}
    \caption{The main Evaluation function.}
    \label{fig:Evaluation2}
\end{figure}

\para The Evaluation function retrieves the serialized model, parameters and run ID from Redis 
via a Direct-Redis connection, deserializing the model with Pickle. The testing dataset is 
retrieved and the MLFlow run is resumed to then make predictions using the model. Using the 
eval\_metrics function, the metrics obtained are output to the console (viewable via the DAG's logs page)
and also logged in the MLFlow run. The model is then saved and the MLFlow is ended, completing the pipeline.

\section{Completed Airflow DAGs}
Two DAGs were created as part of the pipeline. "DockerContainers" consists of two BashOperators 
to start the two Docker containers for MariaDB Columnstore and Redis, whereas "MLOpsPipeline" 
consists of the five tasks of the pipeline. Figure \ref{fig:DAGsList} shows the list of DAGs, 
containing only these two\footnote{Normally, Airflow includes a large list of examples, though these were 
removed by changing the configuration file located at \texttildelow/airflow/airflow.cfg.} DAGs.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/Airflow/DAGs/1.png}
    \caption{The list of DAGs in Airflow.}
    \label{fig:DAGsList}
\end{figure}

\section{Pipeline}
Figure \ref{fig:PipelineGraphDAG} shows the completed Airflow DAG's graph view, which is a graphical 
representation of the sequence of tasks. The five tasks of the pipeline are shown, 
though deployment is not one of them, which is further explained in Section \ref{sec:ImpDeployment}.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/Airflow/DAGs/PipelineGraph.png}
    \caption{The graph view of the completed MLOpsPipeline DAG.}
    \label{fig:PipelineGraphDAG}
\end{figure}

The code for the completed DAG consists of four PythonOperators, each to call the respective main 
function of each stage as detailed in their respective section, as well as a GreatExpectationsOperator 
to run data validation between ingestion and preprocessing. It also declares some variables that are passed 
to each function, which can be easily modified.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/.Code/PipelineDAG.png}
    \caption{The complete code of the MLOpsPipeline DAG.}
    \label{fig:PipelineDAGCode}
\end{figure}


\section{Model Deployment}\label{sec:ImpDeployment}
Deployment is performed separately from the rest of the stages in this pipeline, 
as it involves the hosting of a Uvicorn server on port 8000 to run the code written 
in the Model API script using FastAPI. Through running the server using the command 
"uvicorn modelAPI:main --reload" as detailed in Figure 
\ref{fig:UviStartup}, the "docs" page of FastAPI can be accessed to manually perform POST 
requests to get predictions from the model. The "--reload" argument means that whenever the 
modelAPI Python file is updated, the server will automatically update it with the changes.

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/FastAPI+Uvi/Startup.png}
    \caption{Starting the Uvicorn server on port 8000.}
    \label{fig:UviStartup}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/FastAPI+Uvi/LoadModel.png}
    \caption{Loading the pickled model to make predictions with.}
    \label{fig:UviLoadModel}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.8\linewidth]{Implementation/FastAPI+Uvi/InputAndPrediction.png}
    \caption{The FastAPI classes used for input and predictions.}
    \label{fig:UviClasses}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{Implementation/FastAPI+Uvi/Code.png}
    \caption{The FastAPI endpoints for getting model predictions.}
    \label{fig:UviCode}
\end{figure}




\section{Potential security enhancements}\label{sec:SecurityEnhancements}
In relation to Model Deployment, it is important to note that the use of the "docs" page is not suitable for an actual production 
environment, as it can be manipulated by threat actors to possibly give them direct access to the 
server's backend, meaning it is normally disabled. An actual production environment would link their 
model to their actual web page with forms and text entry fields to input data, though this would not 
have been feasible for this particular project due to time constraints as well as not owning a domain 
to host the site on, meaning it is restricted to only localhost.


\section{Development challenges and future improvements}
During the implementation of the pipeline, the initial package used for serializing data was PyArrow.
However, as development continued, it was discovered that many of PyArrow's useful functions are now 
deprecated, and have been since 2021. This made serializing data and passing it between DAG stages considerably more difficult, 
and for this reason an alternative package was identified: Direct-Redis. Direct-Redis builds on top of the Redis 
package, but will instantly serialize and deserialize data when set() or get() is called, greatly expediting 
the process of storing data in and retrieving data from Redis stores. \autocite{direct-redis_direct-redis_nodate}.
This resolved the serialization issue, meaning the pipeline worked smoothly without any errors, and also 
resulted in much cleaner and easily-readable code. An alternative which would have also worked would have been to 
use Pickle for all serialization rather than just that of the model, which would have been functionally identical.

\para Additionally, during the preprocessing stage, a minor improvement could have been made with the One-Hot 
encoding of the "previous\_loan\_defaults\_on\_file" column by removing the "Yes" column it produces, because 
if one is 0, the other must be 1, meaning that two columns are not needed.

\para Furthermore, the DAG that starts the Docker containers could also be extended to add a BashOperator to run 
"mlflow ui" to initialise the MLFlow server. Currently, just running the DockerContainers DAG followed by the pipeline DAG will 
time out unless MLFlow was already running because the Training and Evaluation phases are unable to connect to MLFlow 
if it is not running.

\para Finally, it could theoretically be possible to incorporate deployment within the DAG through the use of a 
BashOperator to run "uvicorn modelAPI:app --reload". However, this may lead to issues with directories which will be 
looked into, as well as the DAG never actually ending, and running perpetually because the Uvicorn server will not stop 
by itself. On the topic of deployment, the user needing to enter 24 columns of data as a result of One-Hot encoding 
increasing the dimensionality is poor, and encoding the data \textbf{after it has been input} by the user is being considered so 
that the user only needs to enter the original 13 columns which will be silently encoded in the background for the model's use.

\para These issues will be resolved in the final version of the pipeline for January 10, and this section will either 
be removed or rewritten for that version of the report.

\chapter*{Conclusion}
\addcontentsline{toc}{chapter}{Conclusion}
% REWRITE THE INTRODUCTION AFTER THIS.

% =========================================================================
% MAIN NOTES:
% 
% You need to put figures of all these screenshots in that were taken last night.
% That's screenshots from the following folders:
%   Entire .Code folder   
%   Airflow/DAGs folder 
%   Entire FastAPI+Uvi folder
%   MLFlow/Other folder.
% Uvi Code doesn't have the GET function completed? Maybe it does but I don't think so.
% Screenshot of GX Operator install needed.

% Your ingestion isn't really ingesting. Import Kaggle and download + extract the zip through the Kaggle package and a ZIP extractor package.
% Misunderstanding of Label and Ordinal encoding. Pinned a bookmark to Firefox explaining the two.
% Alternatively, with one-hot encoding, just remove the Previous_loan_defaults_no column, as if that's 0, the other must be 1.
% You may have been able to label encode the whole dataset, as random forest can apparently work well with it anyway.]

% Maybe transform the data within the modelAPI script so you don't need to enter 24 columns for a prediction.

% Detailed description on why Arrow didn't work and why Direct-Redis did. Why is Direct-Redis a pip install?
% Add a BashOperator for mlflow ui.
% Data analysis is worth 20%. Consider investing more into that section and/or moving it into the implementation chapter.

% Pipeline plan = 20%
% Implementation & Deployment = 20%
% Application & Analysis = 20%
% Evaluation & Communication of wider issues = 20%

% Remember that some of these can be implemented for Jan 10 rather than now.
% Deadline extended to Dec 19. Coincides too heavily with CMP6202 so not feasible.
% I still THOROUGHLY AND HEAVILY recommend you do it for DEC 13!
% While you could submit and proofread on 19, do not plan around leaving it until then.

% You've said the docs page is a security threat. That's true, but give a source to say that.

% Word count is likely to be approx 11,000, though there's not a massive amount you can do
% about that without slashing entire sections.

% =========================================================================
\include{AppendixA}


\printbibliography

\end{document}

