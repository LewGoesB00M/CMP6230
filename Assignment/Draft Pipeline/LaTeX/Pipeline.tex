% -------------------------------------------------------------------------------
% Establish page structure & font.
\documentclass[12pt]{report}

\usepackage[total={6.5in, 9in},
	left=1in,
	right=1in,
	top=1in,
	bottom=1in,]{geometry} % Page structure

\usepackage{graphicx} % Required for inserting images
\graphicspath{{../../.images/}} % Any additional images I use (BCU logo, etc) are from here.

\usepackage[utf8]{inputenc} % UTF-8 encoding
\usepackage[T1]{fontenc} % T1 font
\usepackage{float}  % Allows for floats to be positioned using [H], which correctly
                    % positions them relative to their location within my LaTeX code.
\usepackage{subcaption}

% -------------------------------------------------------------------------------
% Declare biblatex with custom Harvard BCU styling for referencing.
\usepackage[
    useprefix=true,
    maxcitenames=3,
    maxbibnames=99,
    style=authoryear,
    dashed=false, 
    natbib=true,
    url=false,
    backend=biber
]{biblatex}

% Additional styling options to ensure Harvard referencing format.
\renewbibmacro*{volume+number+eid}{
    \printfield{volume}
    \setunit*{\addnbspace}
    \printfield{number}
    \setunit{\addcomma\space}
    \printfield{eid}}
\DeclareFieldFormat[article]{number}{\mkbibparens{#1}}

% Declare it as the bibliography source, to be called later via \printbibliography
\addbibresource{pipeline.bib}

% -------------------------------------------------------------------------------
% To prevent "Chapter N" display for each chapter
\usepackage[compact]{titlesec}
\usepackage{wasysym}
\usepackage{import}

\titlespacing*{\chapter}{0pt}{-2cm}{0.5cm}
\titleformat{\chapter}[display]
{\normalfont\bfseries}{}{0pt}{\Huge}

% -------------------------------------------------------------------------------
% Custom macro to make an un-numbered footnote.

\newcommand\blfootnote[1]{
    \begingroup
    \renewcommand\thefootnote{}\footnote{#1}
    \addtocounter{footnote}{-1}
    \endgroup
}

% -------------------------------------------------------------------------------
% Fancy headers; used to show my name, BCU logo and current chapter for the page.
\usepackage{fancyhdr}
\usepackage{calc}
\pagestyle{fancy}

\setlength\headheight{37pt} % Set custom header height to fit the image.

\renewcommand{\chaptermark}[1]{%
    \markboth{#1}{}} % Include chapter name.


% Lewis Higgins - ID 22133848           [BCU LOGO]                [CHAPTER NAME]
\lhead{Lewis Higgins - ID 22133848~~~~~~~~~~~~~~~\includegraphics[width=1.75cm]{BCU}}
\fancyhead[R]{\leftmark}

% ------------------------------------------------------------------------------
% Used to add PDF hyperlinks for figures and the contents page.

\usepackage{hyperref}

\hypersetup{
    colorlinks=true,
    linkcolor=black,
    filecolor=magenta,
    urlcolor=blue,
    citecolor=black,
}

% ------------------------------------------------------------------------------
\usepackage{xcolor} 
\usepackage{colortbl}
\usepackage{longtable}
\usepackage{amssymb}
% ------------------------------------------------------------------------------
\usepackage{tcolorbox}
% -------------------------------------------------------------------------------

\title{CMP6230 Draft Pipeline}
\author{Lewis Higgins - Student ID 22133848}
\date{November 2024}

% -------------------------------------------------------------------------------

\begin{document}


\makeatletter
\begin{titlepage}
    \begin{center}
        \includegraphics[width=0.7\linewidth]{BCU}\\[4ex]
        {\huge \bfseries  \@title }\\[50ex]
        {\@author}\\[30ex]
    \end{center}
\end{titlepage}
\makeatother
\thispagestyle{empty}
\newpage


% Page counter trick so that the contents page doesn't increment it.
\setcounter{page}{0}

\tableofcontents
\thispagestyle{empty}


\chapter{Candidate Data Sources}
For the first stage of the pipeline, data ingestion, three data sources will be identified in order to find 
the one that would be most optimal for the production and deployment of a machine learning model to complete 
a supervised learning task.

\section{Candidate 1 - Indian Liver Patient Dataset}
\href{https://archive.ics.uci.edu/dataset/225}{This dataset} \autocite{bendi_ramana_ilpd_2022} consists of real data sourced from hospitals northeast of Andhra Pradesh in India. It was obtained from the
UCI Machine Learning Repository, and has been previously used by \textcite{straw_investigating_2022} in their analysis of sex-related bias in supervised learning models. The UCI ML Repository is a popular host of datasets used by students, 
educators and researchers worldwide for machine learning \autocite{uci_machine_learning_repository_about_nodate}, and hosts these datasets 
on the cloud for public download and usage, as long as credit is given.

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{ILPD-UCI.png}
    \caption{A snapshot of the dataset's UCI repository page.}
    \label{fig:ILPD-UCI}
\end{figure}

This dataset in particular aims to assist in the diagnosis of liver
disease due to increasing mortality rates from conditions like liver cirrhosis, and contains 584 records with 10 features
as well as the "Selector" classification column, where those wihout liver disease are classed as 1, and those with liver disease 
are classed as 2. For the purposes of the ML model, these can be changed to 0 and 1 respectively. 
The dataset is a single flat-file Comma-Seperated Values (CSV) file, which stores data by seperating each column with commas
and each row with line breaks. This CSV file uses a One Big Table (OBT) schema, as seen in the entity relationship diagram 
in Figure \ref{fig:ILPD-ERD}, wherein all of the data within this dataset is stored in a single table. 
The OBT schema is a denormalised schema that is useful for simple querying due to there being no need for table joins. 
However, it is prone to data duplication and redundancy, which can increase necessary storage requirements.

Descriptions of the columns in the dataset, as well as the associated data types, can be found in Table \ref{tab:ILPD-Types}.

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{ILPD-ERD.png}
    \caption{An entity relationship diagram of the Indian Liver Patient Dataset.}
    \label{fig:ILPD-ERD}
\end{figure}


A minor issue with this file is that it has no headers in its CSV file, meaning that when imported, Pandas will interpret the first 
row of data as the names of the columns, though this can be combated by adding the "names" argument when calling Pandas' "read\_csv" function,
seen below in Figure \ref{fig:pandasNames}. 

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.75\textwidth}
       \includegraphics[width=1\linewidth]{pandasNoNames.png}
       \caption{Importing without supplying column names.}
       \label{fig:pandasNames} 
    \end{subfigure}
    
    \begin{subfigure}{1\textwidth}
       \includegraphics[width=1\linewidth]{pandasNames.png}
       \caption{Importing with the column names.}
       \label{fig:PN2}
    \end{subfigure}
    \caption{Importing the erroneous CSV using Pandas. The column headers are highlighted in a red box.}
\end{figure}

A preliminary analysis of the file to ascertain the data types of each column, seen in Figure \ref{fig:ILPD-DTypes}, also revealed that there were 4 missing values in the A/G ratio column.
It is possible that these missing values could be imputed rather than deleted, as it may be possible to calculate what the A/G ratio of these rows would have been in the 
Data Preprocessing stage of a pipeline.

\begin{figure}[H]
    \centering
    \includegraphics[width=.3\linewidth]{pandas/ILPD-DTypes.png}
    \caption{The data types of the Indian Liver Patient Dataset.}
    \label{fig:ILPD-DTypes}
\end{figure}

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.75\textwidth}
        \centering
       \includegraphics[width=0.3\linewidth]{pandas/ILPD-NA.png}
       \caption{Four missing values are identified.}
       \label{fig:NAs1} 
    \end{subfigure}
    
    \begin{subfigure}{1\textwidth}
        \centering
       \includegraphics[width=1\linewidth]{pandas/ILPD-NAValues.png}
       \caption{The four rows in question.}
       \label{fig:NAs2}
    \end{subfigure}
    \caption{The identification of four missing values in the A/G ratio column.}
\end{figure}


\begin{table}[H]
    \centering
        \begin{tabular}{ |p{0.2\textwidth}| p{0.4\textwidth}| p{0.2\textwidth}|}
            \hline
            \cellcolor{blue!25}Column & \cellcolor{blue!25}Description & \cellcolor{blue!25}Measurement level\\
            \hline
            Age & The patient's age. \textbf{Ages of 90 or over were listed as 90 before this dataset was published.} 
            & Ratio\\
            \hline
            Gender & The patient's gender, either "Male" or "Female". & Nominal\\
            \hline
            TB & Total bilirubin. Bilirubin is a substance produced by the liver, and a high presence of it may be indicative of
            liver problems \autocite{mayo_clinic_bilirubin_nodate}. & Ratio\\
            \hline
            DB & Direct bilirubin. This is a slightly different form of bilirubin that is formed after the liver has processed it.
            & Ratio\\
            \hline
            Alkphos & Levels of alkaline phosphate - an enzyme in the body produced by the liver. Too much may indicate liver disease. \autocite{clevelandclinic_alkaline_nodate}
            & Ratio\\
            \hline
            Sgpt & Another enzyme found in the liver, where too much can indicate liver problems.
            & Ratio\\
            \hline
            Sgot & Levels of AST in the blood, where too much indicates liver problems.
            & Ratio\\
            \hline
            TP & Total proteins.
            & Ratio\\
            \hline
            ALB & Albumin - a protein in blood plasma. Too little of this may indicate liver problems.
            & Ratio\\
            \hline
            A/G Ratio & The ratio of albumin to globulin, which is another blood protein.
            & Ratio % IS THIS RATIO?
            \\
            \hline
            Selector & The classifier, indicating if the person has liver disease. The target column for the ML model.
            & Nominal\\
            \hline
    \end{tabular}
    \caption{The descriptions of each column in the Indian Liver Patient Dataset.}\label{tab:ILPD-Types}
\end{table}

This dataset could be used to solve a binary classification problem using the ten predictor 
variables and the ground truth Selector column, which will be used in measuring the accuracy of the model. There is a clear 
positive purpose for developing such a model; as previously mentioned, mortality rates from liver disease are high, and an early
diagnosis that could leverage the power of machine learning can greatly enhance the odds of successful treatment.


% \section{Candidate 2 - Cardiovascular Disease Dataset}
% The second dataset is also a real dataset of heart disease patients from four areas worldwide: Ohio, Hungary, Switzerland, and Virginia,
% and was identified through the UCI ML repository. The dataset consists of 14 features and a classifier column, with the data types and descriptions of each 
% shown in Table \ref{tab:Heart-Types}. The overall purpose of this dataset is for the analysis and diagnosis of heart disease, which is the leading 
% cause of death for both men and women in the US \autocite{cdc_heart_2024}. Unlike Candidate 1, this dataset consists of four seperate CSV files, where each CSV is for one of the 
% aforementioned locations. 

% \begin{table}[H]
%     \centering
%         \begin{tabular}{ |p{0.2\textwidth}|p{0.2\textwidth}| p{0.35\textwidth}|}
%             \hline
%             \cellcolor{blue!25}Column & \cellcolor{blue!25}Format & \cellcolor{blue!25}Description\\
%             \hline
%             Patient Identification Number & Integer & The patient's unique ID. This won't be used as a predictor because it has no affiliation. \\
%             \hline
%             Age & Integer & The patient's age.\\
%             \hline
%             Gender & Integer (Binary) & The patient's gender, where 0 is female and 1 is male. \\
%             \hline
%             Chest pain type & Integer (Nominal) & The type of pain the patient has, where 0 is typical angina, 1 is atypical angina, 2 is pain that is not angina, and 3 is no pain.\\
%             \hline
%             Resting blood pressure & Integer & The patient's blood pressure (It is not stated if this is their systolic or diastolic pressure).\\
%             \hline
%             Serum cholesterol & Integer & Total cholesterol in the blood. Too much can clog arteries and cause heart disease and/or attacks.\\
%             \hline
%             Fasting blood sugar & Integer (Binary) & 0 if under 120mg/dl, 1 if over 120mg/dl.\\
%             \hline
%             Resting ECG results & Integer (Nominal) & 0 if normal, 1 if ST-T wave abormality (irregular beat), 2 if ventricular hypertrophy.\\
%             \hline
%             Maximum heart rate & Integer & The highest heart rate documented for the patient.\\
%             \hline
%             Exercise induced angina & Integer (Binary) & 0 if they do not get chest pain during exercise, 1 if they do.\\
%             \hline
%             Oldpeak & Float & The difference in ST depression (a particular part of the heart's beating motion) when exercising compared to resting.\\
%             \hline
%             ST segment slope & Integer (Nominal) & The trend in the patient's ST segment reading, where 1 is upsloping, 2 is flat and 3 is downsloping. \\
%             \hline
%             Classifier & Integer (Binary) & If the patient has heart disease or not (0 if no, 1 if yes).\\
%             \hline
%     \end{tabular}
%     \caption{The data types of each column in the Cardiovascular Disease Dataset.}\label{tab:Heart-Types}
% \end{table}
\pagebreak
\section{Candidate 2 - Loan Approval Classification Dataset}\label{sec:Dataset}
\href{https://www.kaggle.com/datasets/taweilo/loan-approval-classification-data}{This dataset} \autocite{lo_loan_nodate} was sourced from Kaggle's cloud servers under 
an Apache 2.0 license, which states that the dataset can be used as long as credit is given to the original author,
and takes the form of a flat-file CSV using a One Big Table schema. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{Loan-Kaggle.png}
    \caption{A snapshot of the Loan dataset's Kaggle page.}
    \label{fig:Loan-Kaggle}
\end{figure}

Unlike Candidate 1, this dataset does not consist of real data, and 
instead consists of synthetic data. This is likely due to the fact that this dataset, if it used real data, would contain extremely personal 
information that could not be shared online due to legislation such as GDPR. This particular dataset is an enhanced version of \href{https://www.kaggle.com/datasets/laotse/credit-risk-dataset}{a different credit risk dataset},
which also did not provide an original source and is also presumably synthetic data. The dataset consists of 45,000 records and 14 features, with 
one of these being the ground truth target variable "loan\_status", which is whether the person should be given a loan or not. 
31 notebooks on Kaggle created by the site's users utilise this dataset, with many of these choosing to solve the binary classification 
problem that it presents. The data types for each column can be seen in the entity relationship diagram and Pandas code in Figures \ref{fig:Loan-ERD} and \ref{fig:ILPD-DTypes}, and 
descriptions of each column can be seen in Table \ref{tab:Loan-Types}.

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{Loan-ERD.png}
    \caption{An entity relationship diagram of the Loan Approval Classification Dataset.}
    \label{fig:Loan-ERD}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.6\linewidth]{pandas/Loan-DTypes.png}
    \caption{The data types of the Loan Approval Classification Dataset.}
    \label{fig:Loan-DTypes}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.7\linewidth]{pandas/Loan-NA.png}
    \caption{No missing values in the dataset.}
    \label{fig:Loan-NAs}
\end{figure}


\begin{table}[H]
    \centering
    \begin{tabular}{|p{0.4\textwidth}| p{0.4\textwidth}| p{0.15\textwidth} |}
        \hline
        \cellcolor{blue!25}Column & \cellcolor{blue!25}Description & \cellcolor{blue!25}Measurement level\\
            \hline
            person\_age & The age of the person. & Ratio\\
            \hline
            person\_gender & The person's gender. & Nominal\\
            \hline
            person\_education & The person's highest level of education. & Ordinal\\
            \hline
            person\_emp\_exp & The person's years of employment experience. & Ratio\\
            \hline
            person\_home\_ownership & Home ownership status (for example rent, own, mortgage)
            & Nominal\\
            \hline
            loan\_amnt & The amount of money requested. & Ratio\\
            \hline
            loan\_intent & The purpose of the loan. & Nominal\\
            \hline
            loan\_int\_rate & The interest rate of the loan. & Ratio\\
            \hline
            loan\_percent\_income & Loan amount as a percentage of the person's yearly income.
            & Ratio\\
            \hline
            cb\_person\_cred\_hist\_length & Length of credit history in years. & Ratio\\
            \hline
            credit\_score & Credit score of the person. & Ratio\\
            \hline
            previous\_loan\_defaults\_on\_file & If the person has defaulted on a loan before.
            & Nominal \\
            \hline
            loan\_status & Whether the loan should be approved. 1 if yes, 0 if no.
            & Nominal\\
            \hline
    \end{tabular}
    \caption{The descriptions of each column in the dataset.}\label{tab:Loan-Types}
\end{table}

This dataset is also frequently updated, with its most recent update occurring on the 29th October. 

\pagebreak

\section{Candidate 3 - Spotify Likes Dataset}
\href{https://www.kaggle.com/datasets/bricevergnou/spotify-recommendation/data}{This dataset} was sourced from \href{https://www.kaggle.com/datasets}{Kaggle}, a platform similar to the UCI ML repository in its purpose for students and researchers
that acts as a search engine for datasets, but also allows its users to host competitions, upload their machine learning models, and also upload 
their own Python notebooks. This dataset is stored on their servers on the cloud, and is free to download and use.

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{Spotify-Kaggle.png}
    \caption{A snapshot of the Spotify dataset's Kaggle page.}
    \label{fig:Spotify-Kaggle}
\end{figure}

The data itself is split over 
two JavaScript Object Notation (JSON) files, but also fully present in an included CSV file, with all three utilising a One Big Table schema. The 
download also includes two Python files, which have the JSON data stored in Python dictionaries for ease of access, though these will not be used in 
this brief analysis. JSON files store data in \textbf{key-value pairs}, such as in the example snippet of this dataset depicted in Figure \ref{fig:spotifySnippet}.  

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{spotifySnippet.png}
    \caption{A snippet of the JSON data, viewed in Visual Studio Code.}
    \label{fig:spotifySnippet}
\end{figure}

Every row in the JSON files is part of the single "audio\_features" key, and each new row is seperated by curly braces \{\}. Each column is then given as a 
key-value pair, such as the first row in Figure \ref{fig:spotifySnippet}, where "danceability" is the key, and 0.352 is the associated value.

\noindent This dataset does consist of real data, sourced from the author's personal liked songs directly via the 
\href{https://developer.spotify.com/documentation/web-api}{Spotify API}. There are 195 rows of data, with 100 liked songs, and 95 disliked songs.
Liked and disliked songs are seperated into two JSON files, named "dislike" and "good". The two JSON files have 18 features, as depicted in Figure 
\ref{fig:JSON-ERD}. 

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{SpotifyJSON-ERD.png}
    \caption{An entity relationship diagram of the two JSON files. Data does not overlap between them, so they have no relation.}
    \label{fig:JSON-ERD}
\end{figure}

This dataset has been used to create machine learning models before, most notably by its own author, who has a public Github repository 
showcasing their work \autocite{brice-vergnou_brice-vergnouspotify_recommendation_2024}. 
Before publicising this data, however, the author had done some preprocessing of their own, having included the additional CSV file,
produced as a result of merging the two JSON files into one CSV and removing unnecessary columns, as depicted in Figure \ref{fig:Spotify-ERD}.
Therefore, my preliminary Pandas analysis of the data types and missing values will only be performed on this CSV, seen in Figures \ref{fig:Spotify-DTypes}
and \ref{fig:Spotify-NA}.

\begin{figure}[H]
    \centering
    \includegraphics[width=.5\linewidth]{Spotify-ERD.png}
    \caption{An entity relationship diagram of the preprocessed CSV file.}
    \label{fig:Spotify-ERD}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.4\linewidth]{pandas/Spotify-DTypes.png}
    \caption{The data types of the Spotify Likes Dataset.}
    \label{fig:Spotify-DTypes}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=.4\linewidth]{pandas/Spotify-NA.png}
    \caption{No missing values in the dataset.}
    \label{fig:Spotify-NA}
\end{figure}

While a machine learning classification problem can definitely be performed on this dataset to identify if the author would like a song, 
it has significantly less of a positive impact than Candidates 1 and 2, as this dataset is the author's subjective belief rather than objective
fact that can be applied to other people. Nevertheless, the descriptions of each column can be found in Table \ref{tab:Spotify-Types}.

\begin{table}[H]
    \centering
    \begin{tabular}{ |p{0.2\textwidth}| p{0.45\textwidth}| p{0.2\textwidth}|}
        \hline
        \cellcolor{blue!25}Column & \cellcolor{blue!25}Description & \cellcolor{blue!25}Measurement level\\
            \hline
            Danceability & How suitable a song is for dancing, calculated from the tempo, rhythm stability, beat strength and overall regularity. 1.0 means it is very danceable.
            & Ratio \\
            \hline
            Energy & The intensity and activity of a song. For example, death metal is high energy, whereas classical music is low intensity. 1.0 is the most energetic.
            & Ratio\\
            \hline
            Key & The musical key the song is in, converted to an integer using \href{https://smbutterfield.github.io/ibmt17-18/22-intro-to-non-diatonic-materials/b2-tx-pcintnotation.html}{standard pitch class notation.}\autocite{butterfield_22b_nodate} 
            & Ratio\\
            \hline
            Loudness & The averaged decibel volume of a song, typically between -60 and 0 dB.
            & Interval\\
            \hline
            Mode & Whether a song is in major or minor scale. 1 is major and 0 is minor.
            & Nominal\\
            \hline
            Speechiness & The calculated presence of spoken words in a song.
            & Ratio\\
            \hline
            Acousticness & A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.
            & Ratio\\
            \hline
            Instrumentalness & Whether a song has no vocals.
            & Ratio\\
            \hline
            Liveness & Whether a live audience can be heard as part of a song.
            & Ratio\\
            \hline
            Valence & The musical positiveness of a song.
            & Ratio\\
            \hline
            Tempo & The beats per minute of a song.
            & Ratio\\
            \hline
            Duration\_MS & The duration of a song in milliseconds.
            & Ratio\\
            \hline
            Time signature & The estimated time signature of the song.
            & Ratio\\
            \hline
            \cellcolor{red!15}Liked & The target variable, indicative of whether the author liked the song or not.
            & Ratio\\
            \hline
            \cellcolor{green!15}Type & Always "audio\_features". Not a relevant predictor.
            & Nominal\\
            \hline
            \cellcolor{green!15}ID & Spotify's own unique ID for a song. Not a relevant predictor.
            & Nominal\\
            \hline
            \cellcolor{green!15}URI & Spotify's URI for the song. Not a relevant predictor.
            & Nominal\\
            \hline
            \cellcolor{green!15}Track HREF & A link to the song on Spotify's API. Not a relevant predictor.
            & Nominal\\  
            \hline
            \cellcolor{green!15}Analysis URL & A link to the song's audio analysis data. Not a relevant predictor. 
            & Nominal\\
            \hline
    \end{tabular}
    \caption{The descriptions of each column in the Spotify songs dataset \autocite{spotify_web_nodate}. Red columns are only present in the CSV, whereas green columns are only present in the JSONs.}\label{tab:Spotify-Types}
\end{table}

These measurements and the descriptions are \href{https://developer.spotify.com/documentation/web-api/reference/get-audio-features}{part of Spotify's API},
and are automatically calculated when songs are uploaded to the service. The ground truth of the dataset is present in the CSV file as the "liked" classifier 
column, and a train/test split can be implemented for predictions, which is aided by the fact that this dataset is well balanced (100 liked to 95 disliked).
However, its small size and the fact that the model would only be able to predict one person's specific music taste make 
it a poor candidate.

\section{Chosen dataset}
Of the three candidates presented, the most suitable for a machine learning operations pipeline would be Candidate 2, the loan approval dataset. As mentioned 
in Section \ref{sec:Dataset}, this dataset possesses many predictor variables and an adequate amount of data to train a supervised learning classification model 
to classify whether an individual should be allowed a loan or not. While the data in this dataset is synthetic due to its real equivalent being highly protected 
under data protection legislation, the model trained from said synthetic data could be applied to real data using what it has learned, and could greatly expedite 
the process of loan approvals.

As previously mentioned, the dataset is hosted on Kaggle's cloud database in a CSV file. Libraries such as Pandas
natively work with these types of files which will allow for quick ingestion. However, when the dataset has been ingested,



\chapter{Planning the MLOps Pipeline}\label{ch:MLOps}
All machine learning operations (MLOps) follow a five-step repeatable pipeline, outlined in Figure \ref{fig:MLPipeline}, where the output of one stage
becomes the input of the next. 
\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{MLPipeline.png}
    \caption{The five key steps in an MLOps pipeline \autocite{incycle_software_mlops_nodate}.}
    \label{fig:MLPipeline}
\end{figure}
The pipeline begins with raw data and finishes with a trained machine learning model, and is often 
repeated at certain intervals, which could be as simple as once a day, or it could be repeated as new data becomes available. 
This repetition is performed automatically, so that the final model can become progressively more accurate. Because the process 
must be repeatable and automated, it is essential that data is validated to ensure that one run of the pipeline where the data may have 
been corrupted somehow would not cause issues, which would quickly spiral out of control as the pipeline is repeated again and again.
These validation procedures and the software utilised for them are documented in Section \ref{sec:Software}.
Overall, MLOps pipelines standardise the development and deployment process of machine learning models, ensuring continuous integration
(CI) and continuous delivery (CD) and enhancing collaboration between data scientists and development teams.
\pagebreak

\section{Software to be used in the pipeline}
A wide variety of software will be used across the MLOps pipeline, with an overall glossary of them documented in Table \ref{tab:softwareDescriptions}.
Descriptions on how they will specifically be used in each stage of the pipeline can be found in each stage's respective
section.

\begin{longtable}{ |p{0.2\textwidth}| p{0.8\textwidth}|}
    \hline
    \cellcolor{blue!25}Software/Library & \cellcolor{blue!25}Overview\\
    \hline
    Conda 
    & A package and environment management system which allows 
    for software developers and data scientists to easily control the libraries used in their code. Allows 
    for the creation of \textbf{virtual environments}, which are contained structures where packages can be installed.
    The use of virtual environments allows for specific versions of packages to be kept, for instance with one environment 
    storing an older version of a library for compatibility purposes, with another storing a newer version to be used in a different project. 
    Were it not for these virtual environments, developers would constantly have to uninstall and reinstall specific package versions, wasting 
    considerable amounts of time. Conda also hosts its own repositories to obtain packages from, and a 
    major benefit of Conda occurs during the package installation process, which is that it will identify 
    dependencies and version clashes between packages and solve them automatically, once again saving 
    considerable amounts of time.
    
    In this project, Conda will be used as the environment and package manager to install and contain the 
    libraries used in the development process. To do so, the Miniconda distribution will be installed, as 
    this is a smaller distribution to save hard disk space and download time, but still contains the key 
    Conda backend, as seen in Figure \ref{fig:Miniconda}. \\
    \hline
    Apache Airflow &
    A platform used to orchestrate workflows and pipelines entirely in Python code \autocite{apache_use_nodate}.
    Airflow creates Directed Acyclic Graphs (DAGs), which map out the order and dependencies of each task in a 
    pipeline, and runs tasks in the order specified within the DAG, while also accounting for dependencies.
    Also provides many useful features such as task scheduling, which is especially useful for the automation 
    of an MLOps pipeline, as well as automatic failure handling, where actions can automatically be performed 
    on a task's failure, such as stopping the pipeline to prevent wasting computational resources. Airflow 
    also provides a convenient UI, accessible by using the command "airflow webserver", which will host a 
    web UI where tasks can be run, paused or stopped, as well as viewed in a tree view that maps the 
    sequence and dependencies of tasks. Airflow also provides "Operators", which handle running code such 
    as Python and Bash scripts.

    Airflow will be used within this project at all stages of the pipeline in order to manage the overall 
    execution and structure of tasks. Operators will be used for the execution of all Python or Bash 
    commands throughout the pipeline. By using Airflow in this way, the pipeline can become completely 
    automated with continuous integration and continuous deployment.
    \\
    \hline
    Docker &
    An open-source containerisation system used to enhance the portability of applications by distributing 
    them as self-contained, lightweight instances that come with everything they need to run immediately 
    without needing to worry about system incompatibility (\textcite{aws_what_nodate}, \textcite{docker_docker_2022})
    and minimise issues where software can run on one computer but not another. Docker could be described 
    as working similarly to a virtual machine, though it is considerably more lightweight as it is not 
    virtualising hardware.\\
    \hline
    MariaDB Columnstore &
    A columnar storage engine designed for the processing of petabytes of data with high performance 
    and real-time response regardless of dataset size \autocite{mariadb_mariadb_nodate}. While 
    primarily intended for OLAP databases, it also facilitates OLTP databases. A Docker container 
    for this software will be used in this pipeline.\\
    \hline 
    Redis &
    An in-memory data store used to cache data in a machine's RAM rather than its 
    persistent storage. The benefits of this are that said data can be loaded many times faster than if 
    it were loaded from a hard drive. This does therefore mean that Redis will not be used to store persistent data,
    and it will instead be used to transfer data between different tasks in an Airflow DAG, as Airflow tasks 
    are independent from each other. Redis will solve this by having relevant data loaded into memory before 
    the task ends, at which point it will be read by the following task. A Docker container for this 
    software will be used in this pipeline. Also includes a Python library of the same name that allows 
    for access to the Redis store from Python scripts.\\
    \hline
    Pandas &
    A library used widely in industry for data analytics. It is capable of importing data of various 
    types and storing them in a "DataFrame" object, which preprocessing operations can be performed on.
    It also allows the exporting of data in multiple formats \autocite{pandas_pandas_nodate}.\\
    \hline
    SQLAlchemy &
    A library that allows SQL engine connections to be made from within a Python file. In 
    doing so, data can be read from and stored into SQL databases \autocite{sqlalchemy_sqlalchemy_nodate}.\\
    \hline
    Scikit-learn & 
    A large open-source Python library containing many different functionalities for machine learning \autocite{scikit-learn_scikit-learn_nodate}, 
    such as encoders to convert strings to numerical equivalents, scalers to normalise and standardise 
    the data to reduce variance (described further in Section \ref{sec:Preprocessing}), as well as containing 
    methods to split data into training and testing sets, and fit, train and predict with various different 
    machine learning algorithms (described further in Section \ref{sec:Development}).\\
    \hline
    FastAPI &
    A high-performance framework for developing APIs in Python, proclaimed
    as "One of the fastest Python frameworks available" \autocite{fastapi_fastapi_nodate}.
    Facilitates an API for clients to access the backend ML model later in the pipeline. \\
    \hline
    Uvicorn &
    A low-level webserver implentation in Python with high performance \autocite{uvicorn_uvicorn_nodate}.\\
    \hline
    MLFlow &
    An open-source platform for the tracking and monitoring of machine learning models, 
    storing each iteration of the model so that they may be reproduced at a later point \autocite{mlflow_mlflow_nodate},
    which is especially useful while performing hyperparameter tuning on the model, which is where 
    small details of the model are changed such as the number of decision trees in a Random Forest.
    Also facilitates REST APIs, discussed in Section \ref{sec:Deployment}.\\
    \hline
    Apache Arrow \newline PyArrow &
    Software with a Python library interface that allows for the serialization of objects \autocite{apache_streaming_nodate}. 
    This is important in synergy with Redis and Pandas, as Redis is unable to directly store Pandas dataframes, 
    so they must first be serialized, converting them to a bytes format that Redis can store in memory.\\
    \hline
    Great Expectations &
    A Python library used for data validation, where "expectations" can be set for a dataset, such as 
    all data of a column being numeric, and they will be assessed \autocite{gx_gx_nodate}.\\
    \hline
\caption{Descriptions of software and libraries across the pipeline.}\label{tab:softwareDescriptions}
\end{longtable}

\begin{figure}[H]
    \centering
    \includegraphics[width=.75\linewidth]{miniconda.png}
    \caption{A comparison of the primary Conda distributions \autocite{towardsdatascience_getting_2021}}
    \label{fig:Miniconda}
\end{figure}

\pagebreak

\section{Data Ingestion}
\subsection{Description}
The first step of any machine learning pipeline is data ingestion. This refers to the process of obtaining data from its original source
and transferring it to a relevant storage medium, such as a database or data warehouse, to be used in later stages. This stage 
is undertaken by data scientists.
It is of vital importance that data is not lost or corrupted when it is ingested, as this stage is the baseline for all future stages in the pipeline, and any issues
here will directly impact all future stages, as previously discussed. Though, when ingesting data, it is important to understand what type of system this data 
will be used in, of which there are two options: Online Analytics Processing Systems (OLAP), and Online Transactional Processing Systems (OLTP), as well 
as how it will be loaded into this system, either using an ELT (Extract, Load, Transform) or ETL (Extract, Transform, Load) methodology.

\subsubsection{OLAP and OLTP}

\begin{table}[H]
    \centering
        \begin{tabular}{ |p{0.4\textwidth}| p{0.425\textwidth}|}
            \hline
            \cellcolor{blue!25}OLAP & \cellcolor{blue!25}OLTP\\
            \hline
            Designed for complex queries and data analysis.
            & Designed for lots of short, fast queries ("transactions").\\
            \hline
            Typically store massive amounts of data, sometimes petabytes.
            for extremely detailed analysis. 
            & Usually store less data for speed purposes.\\
            \hline
            Usually historical data, infrequently updated. 
            & Typically real-time data.\\
            \hline 
            Uses database schemas such as star or snowflake schema to allow for queries using many joins. 
            & Uses normalised or denormalised models, such as One Big Table, minimising joins and maximising speed.\\
            \hline
            Slow response times, measured in minutes. 
            & Fast response times, measured in milliseconds.\\
            \hline
    \end{tabular}
    \caption{A comparison of OLAP and OLTP systems \autocite{aws_oltp_nodate}.}\label{tab:OLAP-OLTP}
\end{table}

As mentioned in Table \ref{tab:OLAP-OLTP}, OLAP systems are designed for complex and deep data analytics, which helps 
companies perform tasks such as analysing customer trends, while OLTP systems aim for maximum speed to complete quick transactions, 
which is necessary in situations like processing payments and orders.

\subsubsection{ELT and ETL}
ELT and ETL are both acronyms for the order of processes taken when ingesting data, with "Transform" either 
happening before or after the data is loaded into a storage medium like a data warehouse or data lake. 
The extract phase refers to the intiial gathering of the data from its original source, such as Kaggle for 
the selected dataset in this report. The transform phase refers to early modifications made to the data, such 
as formatting or cleaning, and the load phase refers to the transportation of the data from its original storage
medium (a CSV on Kaggle's cloud servers in this case) to a more optimised and efficient system to be used 
in the execution of the pipeline. More details on said systems can be found in Section \ref{sec:Software}.

Regardless of whether ETL or ELT is used, the data is still extracted, loaded and transformed. However,
the decision of which order to use can be determined from the data itself, with smaller datasets that may need 
complex transformation from their raw form being more suited to ETL, whereas large datasets with less 
transformation needed can be better with ELT \autocite{smallcombe_etl_nodate}.

\begin{table}[H]
    \centering
        \begin{tabular}{ |p{0.4\textwidth}| p{0.425\textwidth}|}
            \hline
            \cellcolor{blue!25}ELT & \cellcolor{blue!25}ETL\\
            \hline
            Data is transformed \textbf{after} being loaded into another storage medium.
            & Data is transformed \textbf{before} being loaded into another storage medium. \\
            \hline
            Useful if the data warehouse is a more modern system with good processing power.
            & Useful if the data warehouse has limited processing abilities of its own,
            typically seen in older systems.\\
            \hline
            Lower latency in the ingestion phase because data is immediately loaded. 
            & Higher latency in the ingestion phase, because the data is being processed first, 
            adding an extra time overhead where the pipeline could be held up.\\
            \hline 
            Simpler to set up due to the immediate loading of the data.
            & Can be complex to set up and maintain, as the transformation would occur outside the warehouse.\\
            \hline
    \end{tabular}
    \caption{A comparison of ETL and ELT methodologies (\textcite{bartley_etl_2024}, \textcite{aws_etl_nodate}).}\label{tab:ELT-ETL}
\end{table}

It can be surmised from the analysis of each method in Table \ref{tab:ELT-ETL} that ETL is best applied 
to older systems with limited processing power, whereas ELT is better in more modern systems. 
It can additionally be argued that performing the ETL order of operations blurs the line between the ingestion and 
preprocessing stages, as the data is transformed before it is actually loaded into the system and
fully ingested, whereas ELT has a clear split between the ingestion and preprocessing of the data. 

\subsection{In this project}
The candidate dataset utilises the One Big Table schema, and it was previously established in Section \ref{sec:Dataset}
that it contains no missing values. The size of the dataset is the largest of the three candidates but is still
small in comparison to those used in industry, being only 3.5MB in comparison to gigabytes and petabytes as previously 
mentioned in Table \ref{tab:OLAP-OLTP}. The data will be ingested using ELT into a MariaDB Columnstore 
OLTP database for quick and efficient loading and querying. To do so, some of the software originally mentioned in 
Table \ref{tab:softwareDescriptions} will be used, seen in Table \ref{tab:IngestionSoftware}.

\pagebreak % REMOVE IF NEEDED


\begin{longtable}{ |p{0.2\textwidth}| p{0.5\textwidth}|}
    \hline
    \cellcolor{blue!25}Software/Library & \cellcolor{blue!25}Usage for ingestion\\
    \hline
    Docker &
    In this stage, Docker will be used to host a container of MariaDB Columnstore.\\
    \hline
    MariaDB Columnstore &
    In this stage, the Columnstore will be hosted on a Docker container at port 3306 \autocite{docker_hub_mariadbcolumnstore_nodate}, 
    and will be used as the OLTP storage medium of the ingested data.\\
    \hline 
    Pandas &
    In the ingestion phase, it will be used to import the dataset's CSV file, and export it as SQL to 
    the MariaDB Columnstore through the use of SQLAlchemy.\\
    \hline
    SQLAlchemy &
    Alongside Pandas, SQLAlchemy will export the DataFrame to the MariaDB Columnstore instance.\\
    \hline
\caption{Software to be used in ingestion.}\label{tab:IngestionSoftware}
\end{longtable}

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{diagrams/Ingestion.png}
    \caption{A diagram of the planned ingestion process.}
    \label{fig:IngestionDiagram}
\end{figure}


\section{Data Preprocessing}\label{sec:Preprocessing}
\subsection{Description}
After the data has been ingested, the preprocessing stage begins, and is often also conducted by 
Data Scientists. This stage encompasses the
cleaning, integration and transformation of the data in order to optimise the dataset for model development.

Cleaning refers to the identification of missing, inaccurate or malformed data within the dataset,
as well as its removal or imputation where possible.

Integration is often seen in datasets with multiple tables or that have been retrieved from multiple sources,
and refers to the combination of the retrieved data into a single flat file. 

Transformation, also known as feature engineering, is a considerable aspect of data preprocessing
which refers to the manipulation and formatting of the data, such as changing the formats of columns 
from numeric dates to proper date data types, as well as the handling of categorical data, such as genders, 
which may originally be strings. Strings cannot be interpreted by machine learning models, and therefore they 
are encoded into numbers using techniques such as label encoding, which converts the unique values in a column
to a numerical representation, such as male being 0 and female being 1. Data is also normalised and standardised 
in this stage, meaning that numerical data is reduced to being between 0 and 1 to adjust the overall scale of the 
data. This is especially useful with algorithms such as K-Nearest Neighbours, where large differences in distance 
between data can mislead the classification algorithm \autocite{ibm_what_2021}.

Once these tasks have all been completed, the dataset will be ready to be used for model development.

\subsection{In this project}
The preprocessing in this project will analyse the ingested data and transform columns such as "person\_gender"
to categorically labelled numerical equivalents. To do so, some of the software and libraries originally mentioned 
in Table \ref{tab:softwareDescriptions} will be used, shown below in Table \ref{tab:PreprocessingSoftware}


\begin{longtable}{ |p{0.2\textwidth}| p{0.5\textwidth}|}
    \hline
    \cellcolor{blue!25}Software/Library & \cellcolor{blue!25}Usage for preprocessing\\
    \hline
    Pandas & 
    Will be used alongside SQLAlchemy to load the dataset from the MariaDB Columnstore instance hosted on Docker, 
    as well as for the actual transformation of the data using the methods provided by Pandas dataframes.\\
    \hline
    SQLAlchemy & 
    Will be used alongside Pandas to query and receive data from the MariaDB Columnstore instance.\\
    \hline
    Scikit-learn & 
    Will be used to normalise and standardise the data, as well as encode any string data into numerical 
    equivalents.\\
    \hline
    Docker &
    Will host the MariaDB Columnstore container. Additionally for this section, it will also host a 
    container for Redis to store the processed dataset.\\
    \hline
    Redis &
    Will store the processed dataframe in memory. Cannot directly store the dataframe as mentioned in 
    Table \ref{tab:softwareDescriptions}, so it must be serialised first using Arrow.\\
    \hline
    Arrow & 
    Will serialise the processed dataframe so that it can then be stored by Redis.\\
    \hline
\caption{Descriptions of software to be used for preprocessing.}\label{tab:PreprocessingSoftware}
\end{longtable}

\begin{figure}[H]
    \centering
    \includegraphics[width=.9\linewidth]{diagrams/Preprocessing.png}
    \caption{A diagram of the planned preprocessing stage.}
    \label{fig:PreprocessingDiagram}
\end{figure}



\section{Model Development}\label{sec:Development}
\subsection{Description}
The model development stage uses the processed dataset from the preprocessing stage and leverages 
machine learning algorithms to solve the problem in question, either a classification problem where 
data will be identified as being of a certain category (class), or a regression problem where unknown 
data can be predicted. Both of these problems require the model to be "fitted" and "trained".
These refer to the utilisation of the processed dataset for the recognition of patterns, 
associations and correlations within the data. To fit and train the data, it is split into two sets:
a training set, consisting of a large majority of the data (\~80\%), and a testing set which uses the 
remaining minority. The algorithm will then use what it has learned from the training set to make predictions 
on the testing set, from which the accuracy of the model can be ascertained. These processes often yield better
results with larger datasets, as the algorithm will have more information to learn and make predictions based on,
which is why it is preferred to not remove data from the dataset unless strictly necessary.

This stage is conducted by machine learning engineers, and its output is that of the trained machine learning 
model, which can then be deployed.


\subsection{In this project}
The candidate dataset poses a classification problem, and as such, the Scikit-Learn Python library 
will be particularly key in this stage for its implementation of a Random Forest algorithm, which is 
reputed as one of the most accurate models in many scenarios. However, it can have a steep processing
time depending on how many decision trees it is told to create. 

\begin{longtable}{ |p{0.2\textwidth}| p{0.5\textwidth}|}
    \hline
    \cellcolor{blue!25}Software/Library & \cellcolor{blue!25}Usage for development\\
    \hline
    Arrow & 
    Will deserialize the stored dataframe from Redis after the preprocessing stage.\\
    \hline
    Pandas & 
    Will store the dataframe deserialized by Arrow.\\
    \hline
    Redis &
    Stores the processed dataset to be retrieved at the beginning of this stage.\\
    \hline
    Scikit-learn & 
    Will be used for its implementations of various algorithms, most notably a Random Forest Classifier.
    Will also provide useful metrics such as accuracy on the training and test data.\\
    \hline
    MLFlow &
    Will be used to store and track each iteration of the model that is produced as this phase is 
    repeated.\\
    \hline
\caption{Descriptions of software to be used for model development.}\label{tab:DevelopmentSoftware}
\end{longtable}


% DO I USE RANDOM FOREST? IT'S CONSIDERED A GOOD ONE FOR CLASSIFICATION.

\section{Model Deployment}\label{sec:Deployment}
\subsection{Description}
The developed model from the previous stage of the pipeline can then be integrated into an actual environment,
and can be utilised as a tool to make decisions. This stage is where software engineers will make the model 
available for use, and will therefore begin to be provided with unseen data, which refers to data outside of
the original training dataset. Models are typically deployed using Representational State Transfer APIs, 
better known as REST APIs \autocite{redhat_what_nodate}. REST APIs make use of typical frontend web 
HTTP requests (GET, POST, PUT, DELETE) from the client to give instructions to the backend machine learning
model \autocite{restfulapi_what_2023}. A significant benefit of using REST APIs is the 
massive portability benefits provided; using a REST API means that the model can provide results to a 
wide variety of devices such as Windows PCs, Macs, and even phones, as anything that can make HTTP requests 
can interface with one.


\subsection{In this project}
The produced model will be hosted on a webserver via Uvicorn, and the REST API will be implemented 
via FastAPI. These two Python packages will provide an interface where the user can input the predictor 
variables of the dataset (age, credit score, etc.) to receive the model's output result.


\begin{longtable}{ |p{0.2\textwidth}| p{0.5\textwidth}|}
    \hline
    \cellcolor{blue!25}Software/Library & \cellcolor{blue!25}Usage for deployment\\
    \hline
    FastAPI &
    Will be used to provide the API between the user and the ML model, where the user will 
    give data to API endpoints and the model will supply a prediction.\\
    \hline
    Uvicorn &
    Will be used to host the webserver that the API will run on.\\
    \hline
    MLFlow &
    Will be used as a "bridge" of sorts between the user and the model, and will access and load 
    it as well as retrieving any necessary artifacts to process the data that the user supplies.\\
    \hline
\caption{Descriptions of software to be used for model deployment.}\label{tab:DeploymentSoftware}
\end{longtable}



\section{Model Monitoring}\label{sec:Monitoring}
\subsection{Description}
After the model is deployed, its performance is continuously monitored by data scientists. The monitoring 
process consists of the analysis of the model's results via metrics such as those found in Table \ref{tab:metrics}
By monitoring the model, any issues with it can be quickly identified and the pipeline can be restarted to 
yield a higher accuracy model, which can be monitored again.  

\begin{longtable}{ |p{0.2\textwidth}| p{0.4\textwidth}| p{0.2\textwidth}|}
    \hline
    \cellcolor{blue!25}Metric & \cellcolor{blue!25}What is it? & \cellcolor{blue!25}Type\\
    \hline
    Accuracy & The number of correct predictions divided by the total amount of predictions. & Classification\\
    \hline
    Precision & The ratio of correctly predicted positives to the total amount of positives in the dataset. 
    & Classification \\
    \hline
    F1-score & A measurement calculated from a model's accuracy and precision.\autocite{kundu_f1_nodate} & Classification\\
    \hline
    Mean Absolute Error (MAE) & The average of the differences between predicted and actual values. & Regression\\
    \hline
    $R^2$ & Also known as the coefficient of determination, shows how well the predicted data fits the 
    actual data \autocite{cfi_r-squared_nodate}. & Regression \\
    \hline
\caption{Descriptions of various metrics to grade ML models.}\label{tab:metrics}
\end{longtable}


\subsection{In this project}
This pipeline aims to produce a binary classification model, so metrics such as F1-score will be useful in the analysis
of each iteration of the model. To do so, MLFlow will be used to store each iteration's parameters and performance
metrics.




% \chapter{Appendix}
% \section{Data schemas}
% \subsection{One Big Table (OBT)}

% \subsection{Star}
% Dimension tables etc
% \subsection{Snowflake}
% Idk

\printbibliography

\end{document}